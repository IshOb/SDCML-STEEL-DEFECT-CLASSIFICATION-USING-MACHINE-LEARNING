{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configs\n",
    "\n",
    "#### NOTE: You may need to change the path to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'C:\\\\Users\\\\HP\\\\Desktop\\\\one-shot-steel-surfaces-master\\\\dataset_all\\\\cnn_data\\\\training'\n",
    "TEST_DIR = 'C:\\\\Users\\\\HP\\\\Desktop\\\\one-shot-steel-surfaces-master\\\\dataset_all\\\\cnn_data\\\\testing'\n",
    "\n",
    "WEIGHT_PATH = './weights/steel/cnn/'\n",
    "SAVE_IMAGE_PATH = './images/steel/cnn/'\n",
    "\n",
    "SAVE_TEST_RESULTS_PATH = './images/steel/test_output'\n",
    "\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available()) else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 80\n",
    "LR = 0.0005\n",
    "N_GPU = 1\n",
    "SET_L = False\n",
    "INPUT_D = 3 if SET_L == False else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(INPUT_D, 4, kernel_size=3),\n",
    "            nn.ReLU(),        \n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(4, 8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.ZeroPad2d(1),\n",
    "            nn.Conv2d(8, 8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(8*100*100, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 6),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img, text=None, figsize=(20,20), save_into=None):\n",
    "    img = img.numpy()\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis('off')\n",
    "    if text:\n",
    "        plt.text(50, 8, text, bbox={'facecolor': 'white', 'alpha': 1 })\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    \n",
    "    if save_into:\n",
    "        plt.savefig(save_into)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_databatch(batch):\n",
    "    \"\"\"\n",
    "    batch: tensor batch from dataset\n",
    "    \"\"\"\n",
    "    print('Each batch of training data is a tuple of {} elements.'.format(len(batch)))\n",
    "    \n",
    "    print('Shape of each element of a tuple in the batch:')\n",
    "    for i in range(len(batch)):\n",
    "        print('element#{}: {}'.format(i, batch[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Custom Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandRotateTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return transforms.functional.rotate(x, angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                            transforms.Resize((100,100)),\n",
    "                            RandRotateTransform(angles=[0, 90, 180, 270]),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each batch of training data is a tuple of 2 elements.\n",
      "Shape of each element of a tuple in the batch:\n",
      "element#0: torch.Size([128, 3, 100, 100])\n",
      "element#1: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "example_batch = next(iter(train_loader))\n",
    "\n",
    "explain_databatch(example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \t loss: 0.000758\n",
      "[2] \t loss: 0.000469\n",
      "[3] \t loss: 0.000563\n",
      "[4] \t loss: 0.000554\n",
      "[5] \t loss: 0.000454\n",
      "[6] \t loss: 0.000488\n",
      "[7] \t loss: 0.000346\n",
      "[8] \t loss: 0.000352\n",
      "[9] \t loss: 0.000391\n",
      "[10] \t loss: 0.000286\n",
      "[11] \t loss: 0.000318\n",
      "[12] \t loss: 0.000369\n",
      "[13] \t loss: 0.000530\n",
      "[14] \t loss: 0.000158\n",
      "[15] \t loss: 0.000226\n",
      "[16] \t loss: 0.000162\n",
      "[17] \t loss: 0.000202\n",
      "[18] \t loss: 0.000112\n",
      "[19] \t loss: 0.000106\n",
      "[20] \t loss: 0.000189\n",
      "[21] \t loss: 0.000228\n",
      "[22] \t loss: 0.000228\n",
      "[23] \t loss: 0.000085\n",
      "[24] \t loss: 0.000065\n",
      "[25] \t loss: 0.000040\n",
      "[26] \t loss: 0.000066\n",
      "[27] \t loss: 0.000041\n",
      "[28] \t loss: 0.000061\n",
      "[29] \t loss: 0.000087\n",
      "[30] \t loss: 0.000067\n",
      "[31] \t loss: 0.000192\n",
      "[32] \t loss: 0.000140\n",
      "[33] \t loss: 0.000045\n",
      "[34] \t loss: 0.000129\n",
      "[35] \t loss: 0.000008\n",
      "[36] \t loss: 0.000092\n",
      "[37] \t loss: 0.000030\n",
      "[38] \t loss: 0.000095\n",
      "[39] \t loss: 0.000086\n",
      "[40] \t loss: 0.000051\n",
      "[41] \t loss: 0.000050\n",
      "[42] \t loss: 0.000063\n",
      "[43] \t loss: 0.000022\n",
      "[44] \t loss: 0.000025\n",
      "[45] \t loss: 0.000023\n",
      "[46] \t loss: 0.000062\n",
      "[47] \t loss: 0.000024\n",
      "[48] \t loss: 0.000007\n",
      "[49] \t loss: 0.000077\n",
      "[50] \t loss: 0.000019\n",
      "[51] \t loss: 0.000113\n",
      "[52] \t loss: 0.000046\n",
      "[53] \t loss: 0.000043\n",
      "[54] \t loss: 0.000026\n",
      "[55] \t loss: 0.000017\n",
      "[56] \t loss: 0.000042\n",
      "[57] \t loss: 0.000070\n",
      "[58] \t loss: 0.000008\n",
      "[59] \t loss: 0.000030\n",
      "[60] \t loss: 0.000003\n",
      "[61] \t loss: 0.000014\n",
      "[62] \t loss: 0.000007\n",
      "[63] \t loss: 0.000014\n",
      "[64] \t loss: 0.000034\n",
      "[65] \t loss: 0.000019\n",
      "[66] \t loss: 0.000037\n",
      "[67] \t loss: 0.000067\n",
      "[68] \t loss: 0.000026\n",
      "[69] \t loss: 0.000066\n",
      "[70] \t loss: 0.000009\n",
      "[71] \t loss: 0.000006\n",
      "[72] \t loss: 0.000014\n",
      "[73] \t loss: 0.000002\n",
      "[74] \t loss: 0.000012\n",
      "[75] \t loss: 0.000048\n",
      "[76] \t loss: 0.000025\n",
      "[77] \t loss: 0.000018\n",
      "[78] \t loss: 0.000034\n",
      "[79] \t loss: 0.000011\n",
      "[80] \t loss: 0.000005\n",
      "[81] \t loss: 0.000009\n",
      "[82] \t loss: 0.000008\n",
      "[83] \t loss: 0.000003\n",
      "[84] \t loss: 0.000005\n",
      "[85] \t loss: 0.000003\n",
      "[86] \t loss: 0.000002\n",
      "[87] \t loss: 0.000006\n",
      "[88] \t loss: 0.000004\n",
      "[89] \t loss: 0.000013\n",
      "[90] \t loss: 0.000004\n",
      "[91] \t loss: 0.000049\n",
      "[92] \t loss: 0.000013\n",
      "[93] \t loss: 0.000015\n",
      "[94] \t loss: 0.000012\n",
      "[95] \t loss: 0.000001\n",
      "[96] \t loss: 0.000010\n",
      "[97] \t loss: 0.000005\n",
      "[98] \t loss: 0.000006\n",
      "[99] \t loss: 0.000024\n",
      "[100] \t loss: 0.000002\n",
      "[101] \t loss: 0.000010\n",
      "[102] \t loss: 0.000021\n",
      "[103] \t loss: 0.000003\n",
      "[104] \t loss: 0.000063\n",
      "[105] \t loss: 0.000001\n",
      "[106] \t loss: 0.000024\n",
      "[107] \t loss: 0.000045\n",
      "[108] \t loss: 0.000007\n",
      "[109] \t loss: 0.000015\n",
      "[110] \t loss: 0.000006\n",
      "[111] \t loss: 0.000000\n",
      "[112] \t loss: 0.000001\n",
      "[113] \t loss: 0.000002\n",
      "[114] \t loss: 0.000000\n",
      "[115] \t loss: 0.000031\n",
      "[116] \t loss: 0.000014\n",
      "[117] \t loss: 0.000002\n",
      "[118] \t loss: 0.000035\n",
      "[119] \t loss: 0.000001\n",
      "[120] \t loss: 0.000013\n",
      "\n",
      "\n",
      "Finished Training!!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(120):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            running_loss = 0.0\n",
    "    print('[%d] \\t loss: %.6f' % (epoch + 1, running_loss / 2000))\n",
    "\n",
    "print('\\n\\nFinished Training!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'C:\\\\Users\\\\HP\\\\Desktop\\\\one-shot-steel-surfaces-master\\\\test_cnnnet.pth')\n",
    "testset = torchvision.datasets.ImageFolder(root=TEST_DIR, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 91 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "rs=0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
